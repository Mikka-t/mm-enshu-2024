{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards: 100%|██████████| 17/17 [28:19<00:00, 99.96s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 17/17 [00:08<00:00,  1.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"elyza/Llama-3-ELYZA-JP-8B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"elyza/Llama-3-ELYZA-JP-8B\")\n",
    "\n",
    "# Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-3B\")\n",
    "\n",
    "\n",
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\", trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\", trust_remote_code=True)\n",
    "\n",
    "# Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/QwQ-32B-Preview\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/QwQ-32B-Preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.02s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig\n",
    "\n",
    "model_name = \"elyza/Llama-3-ELYZA-JP-8B\"\n",
    "# model_name = \"Qwen/Qwen2.5-3B\"\n",
    "# model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "# model_name = \"Qwen/QwQ-32B-Preview\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    # load_in_8bit=True  # 8ビット量子化を指定,\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 #bfloatはfloatに比べて、精度よりも数値が表現できる桁数の幅（ダイナミックレンジ）を重視した設計...らしい\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config, #量子化を行う\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "仕事の熱意を取り戻すためのアイデアを5つ挙げてください。<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "そんなもんねーよハゲ！<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "tensor([[128000, 128006,   9125, 128007,    271,  30591, 112568,  15682, 124097,\n",
      "         103350,  16556, 104622, 106241,  26854, 102433, 107707,  39880,  57207,\n",
      "         105335,  52414,  38641,   1811,  66378,  20230,  64467,  20379,  29295,\n",
      "          43568,  16995, 126513,   5486,  40053,  20230, 102433, 102158,  16556,\n",
      "         113925,  39926,  72315,   1811, 128009, 128006,    882, 128007,    271,\n",
      "         117876,  16144, 107969,  37689,  30512, 108167, 114941,  17663, 124122,\n",
      "         111090,  68408,  39880,  30512,     20,  59739, 123680, 102639,  38144,\n",
      "          72315,   1811, 128009, 128006,  78191, 128007,    271, 107943,  32977,\n",
      "          25827, 101832,  38248,    230, 102040, 105621,   6447, 128009, 128006,\n",
      "          78191, 128007,    271]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。\"\n",
    "text = \"仕事の熱意を取り戻すためのアイデアを5つ挙げてください。\"\n",
    "assistant_output = \"そんなもんねーよハゲ！\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "    {\"role\": \"assistant\", \"content\": assistant_output},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "token_ids = tokenizer.encode(\n",
    "    prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "print(token_ids)\n",
    "# with torch.no_grad():\n",
    "#     output_ids = model.generate(\n",
    "#         token_ids.to(model.device),\n",
    "#         max_new_tokens=1200,\n",
    "#         do_sample=True,\n",
    "#         temperature=0.6,\n",
    "#         top_p=0.9,\n",
    "#     )\n",
    "# output = tokenizer.decode(\n",
    "#     output_ids.tolist()[0][token_ids.size(1):], skip_special_tokens=True\n",
    "# )\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "#学習済みモデルを利用してみる\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" #これによりgpu1がcuda:0として扱われるようになるらしい、複数台利用したいときは0,1などにすればよい\n",
    "from generate_graph_local import Graph_Generator\n",
    "# graph_generator = Graph_Generator(USE_LOCAL_LLM=True,model_name=\"Qwen/Qwen2.5-3B\",tokenizer_name =\"Qwen/Qwen2.5-3B\" )\n",
    "graph_generator = Graph_Generator(USE_LOCAL_LLM=True,model_name=\"Phi-3.5-mini-instruct-lora_ft\",tokenizer_name =\"microsoft/Phi-3.5-mini-instruct\" )\n",
    "# graph_generator = Graph_Generator(USE_LOCAL_LLM=True,model_name=\"elyza/Llama-3-ELYZA-JP-8B\",tokenizer_name =\"elyza/Llama-3-ELYZA-JP-8B\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_content タイトル: 美味しくてびっくり！ローストビーフ♪赤ワインソース レシピ・作り方\n",
      "\n",
      "材料:\n",
      "ローストビーフ用かたまり肉 400ｇ～500ｇ\n",
      "塩 小さじ１/２\n",
      "粗引き黒胡椒 少々\n",
      "ローリエ 2枚\n",
      "ニンニク　スライス １かけ分\n",
      "オリーブオイル 大さじ１\n",
      "★赤ワインソース★ \n",
      "赤ワイン 300ｃｃ\n",
      "玉ねぎ　くし切り １/２個\n",
      "蜂蜜 大さじ２\n",
      "醤油 大さじ２\n",
      "コンソメキューブ 1ケ\n",
      "★バター 10ｇ\n",
      "\n",
      "手順:\n",
      "牛肉に塩、黒コショウをすりこみ、ローリエ、にんにく、オリーブオイル、と一緒にビニール袋に入れ１時間ほど寝かせます。\n",
      "テフロン加工のフライパンで、肉の表面をに焦げ目を付けるように焼きます。(ローリエ、ニンニクも一緒に、)強火で、一面30秒づつ焼きつけます。※トングを使うと簡単です。\n",
      "鍋に赤ワイン、玉ねぎ入れを煮立たせます。アルコールが飛んだら、コンソメ、醤油、蜂蜜を加え溶かします。\n",
      "2の焼いた肉とニンニク、ローリエを加え、クッキングシートのお落しブタと鍋の蓋をして、中火～中弱火で4分加熱します。火を消し蓋をしたまま20分おきます。(予熱で火が通ります)\n",
      "肉を取り出し、アルミホイルに包み冷めるまで待ちます。(熱いうちに切り分けるとドリップが出ます。)\n",
      "(ソースの仕上げ)ローリエ、玉ねぎを取り除き、あくを取りながら２/３くらいまで煮詰めます。仕上げにバターを加えます。器に入れローストビーフに添えます。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "output_str:\n",
      " Knowledge Graph:\n",
      "\n",
      "Node 1: ローストビーフ用かたまり肉\n",
      "Type: ingredient\n",
      "Quantity: 400～500g\n",
      "\n",
      "Node 2: 塩\n",
      "Type: ingredient\n",
      "Quantity: 小さじ1/2\n",
      "\n",
      "Node 3: 粗引き黒胡椒\n",
      "Type: ingredient\n",
      "Quantity: 少々\n",
      "\n",
      "Node 4: ローリエ\n",
      "Type: ingredient\n",
      "Quantity: 2枚\n",
      "\n",
      "Node 5: ニンニク\n",
      "Type: ingredient\n",
      "Quantity: １かけ分\n",
      "\n",
      "Node 6: オリーブオイル\n",
      "Type: ingredient\n",
      "Quantity: 大さじ1\n",
      "\n",
      "Node 7: 赤ワイン\n",
      "Type: ingredient\n",
      "Quantity: 300cc\n",
      "\n",
      "Node 8: 玉ねぎ\n",
      "Type: ingredient\n",
      "Quantity: 1/2個\n",
      "\n",
      "Node 9: 蜂蜜\n",
      "Type: ingredient\n",
      "Quantity: 大さじ2\n",
      "\n",
      "Node 10: 醤油\n",
      "Type: ingredient\n",
      "Quantity: 大さじ2\n",
      "\n",
      "Node 11: コンソメキューブ\n",
      "Type: ingredient\n",
      "Quantity: 1個\n",
      "\n",
      "Node 12: バター\n",
      "Type: ingredient\n",
      "Quantity: 10g\n",
      "\n",
      "Node 13: ローストビーフ\n",
      "Type: intermediate\n",
      "\n",
      "Node 14: 赤ワインソース\n",
      "Type: intermediate\n",
      "\n",
      "Edge 1: ローストビーフ用かたまり肉 - ローストビーフ（焼く）\n",
      "Edge 2: 塩 - ローストビーフ（下味付ける）\n",
      "Edge 3: 粗引き黒胡椒 - ローストビーフ（下味付ける）\n",
      "Edge 4: ローリエ - ローストビーフ（ローストする）\n",
      "Edge 5: ニンニク - 赤ワインソース（煮込む）\n",
      "Edge 6: オリーブオイル - ローストビーフ（焼く）\n",
      "Edge 7: 赤ワイン - 赤ワインソース（溶かす）\n",
      "Edge 8: 玉ねぎ - 赤ワインソース（煮込む）\n",
      "Edge 9: 蜂蜜 - 赤ワインソース（煮込む）\n",
      "Edge 10: 醤油 - 赤ワインソース（煮込む）\n",
      "Edge 11: コンソメキューブ - 赤ワインソース（煮込む）\n",
      "Edge 12: バター - 赤ワインソース（仕上げ）\n",
      "Edge 13: ローストビーフ - ローストビーフ（完成）\n",
      "Edge 14: 赤ワインソース - ローストビーフ（完成）\n",
      "\n",
      "output_json: {'nodes': [{'id': 'ローストビーフ用かたまり肉', 'type': 'ingredient', 'quantity': '400～500g'}, {'id': '塩', 'type': 'ingredient', 'quantity': '小さじ1/2'}, {'id': '粗引き黒胡椒', 'type': 'ingredient', 'quantity': '少々'}, {'id': 'ローリエ', 'type': 'ingredient', 'quantity': '2枚'}, {'id': 'ニンニク', 'type': 'ingredient', 'quantity': '１かけ分'}, {'id': 'オリーブオイル', 'type': 'ingredient', 'quantity': '大さじ1'}, {'id': '赤ワイン', 'type': 'ingredient', 'quantity': '300cc'}, {'id': '玉ねぎ', 'type': 'ingredient', 'quantity': '1/2個'}, {'id': '蜂蜜', 'type': 'ingredient', 'quantity': '大さじ2'}, {'id': '醤油', 'type': 'ingredient', 'quantity': '大さじ2'}, {'id': 'コンソメキューブ', 'type': 'ingredient', 'quantity': '1個'}, {'id': 'バター', 'type': 'ingredient', 'quantity': '10g'}, {'id': 'ローストビーフ', 'type': 'intermediate', 'quantity': None}, {'id': '赤ワインソース', 'type': 'intermediate', 'quantity': None}], 'edges': [{'source': 'ローストビーフ用かたまり肉', 'target': 'ローストビーフ', 'action': '焼く'}, {'source': '塩', 'target': 'ローストビーフ', 'action': '下味付ける'}, {'source': '粗引き黒胡椒', 'target': 'ローストビーフ', 'action': '下味付ける'}, {'source': 'ローリエ', 'target': 'ローストビーフ', 'action': 'ローストする'}, {'source': 'ニンニク', 'target': '赤ワインソース', 'action': '煮込む'}, {'source': 'オリーブオイル', 'target': 'ローストビーフ', 'action': '焼く'}, {'source': '赤ワイン', 'target': '赤ワインソース', 'action': '溶かす'}, {'source': '玉ねぎ', 'target': '赤ワインソース', 'action': '煮込む'}, {'source': '蜂蜜', 'target': '赤ワインソース', 'action': '煮込む'}, {'source': '醤油', 'target': '赤ワインソース', 'action': '煮込む'}, {'source': 'コンソメキューブ', 'target': '赤ワインソース', 'action': '煮込む'}, {'source': 'バター', 'target': '赤ワインソース', 'action': '仕上げ'}, {'source': 'ローストビーフ', 'target': 'ローストビーフ', 'action': '完成'}, {'source': '赤ワインソース', 'target': 'ローストビーフ', 'action': '完成'}]}\n",
      "{'nodes': [{'id': 'ローストビーフ用かたまり肉', 'type': 'ingredient', 'quantity': '400～500g'}, {'id': '塩', 'type': 'ingredient', 'quantity': '小さじ1/2'}, {'id': '粗引き黒胡椒', 'type': 'ingredient', 'quantity': '少々'}, {'id': 'ローリエ', 'type': 'ingredient', 'quantity': '2枚'}, {'id': 'ニンニク', 'type': 'ingredient', 'quantity': '１かけ分'}, {'id': 'オリーブオイル', 'type': 'ingredient', 'quantity': '大さじ1'}, {'id': '赤ワイン', 'type': 'ingredient', 'quantity': '300cc'}, {'id': '玉ねぎ', 'type': 'ingredient', 'quantity': '1/2個'}, {'id': '蜂蜜', 'type': 'ingredient', 'quantity': '大さじ2'}, {'id': '醤油', 'type': 'ingredient', 'quantity': '大さじ2'}, {'id': 'コンソメキューブ', 'type': 'ingredient', 'quantity': '1個'}, {'id': 'バター', 'type': 'ingredient', 'quantity': '10g'}, {'id': 'ローストビーフ', 'type': 'intermediate', 'quantity': None}, {'id': '赤ワインソース', 'type': 'intermediate', 'quantity': None}], 'edges': [{'source': 'ローストビーフ用かたまり肉', 'target': 'ローストビーフ', 'action': '焼く'}, {'source': '塩', 'target': 'ローストビーフ', 'action': '下味付ける'}, {'source': '粗引き黒胡椒', 'target': 'ローストビーフ', 'action': '下味付ける'}, {'source': 'ローリエ', 'target': 'ローストビーフ', 'action': 'ローストする'}, {'source': 'ニンニク', 'target': '赤ワインソース', 'action': '煮込む'}, {'source': 'オリーブオイル', 'target': 'ローストビーフ', 'action': '焼く'}, {'source': '赤ワイン', 'target': '赤ワインソース', 'action': '溶かす'}, {'source': '玉ねぎ', 'target': '赤ワインソース', 'action': '煮込む'}, {'source': '蜂蜜', 'target': '赤ワインソース', 'action': '煮込む'}, {'source': '醤油', 'target': '赤ワインソース', 'action': '煮込む'}, {'source': 'コンソメキューブ', 'target': '赤ワインソース', 'action': '煮込む'}, {'source': 'バター', 'target': '赤ワインソース', 'action': '仕上げ'}, {'source': 'ローストビーフ', 'target': 'ローストビーフ', 'action': '完成'}, {'source': '赤ワインソース', 'target': 'ローストビーフ', 'action': '完成'}]}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import  BeautifulSoup\n",
    "import os\n",
    "\n",
    "\n",
    "saved_html_list = os.listdir(\"./data/recipe_htmls/\")\n",
    "len(saved_html_list)\n",
    "for html in saved_html_list[10:11]:\n",
    "    html = f\"./data/recipe_htmls/{html}\"\n",
    "    with open(html,\"r\",encoding=\"utf-8\") as f:\n",
    "        graph = graph_generator.generate_graph(url=None,html=f)\n",
    "        print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/recipe_graph_jsons/GPT4o.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m         _json \u001b[38;5;241m=\u001b[39m parse_to_json(_json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_str\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     47\u001b[0m         created_recipe_jsons\u001b[38;5;241m.\u001b[39mappend(_json)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/recipe_graph_jsons/GPT4o.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     51\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(created_recipe_jsons,f)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/recipe_graph_jsons/GPT4o.json'"
     ]
    }
   ],
   "source": [
    "#GPT4oの良質なoutputを利用する\n",
    "def parse_to_json(input_string):\n",
    "    # TODO: 色々なパターンのテキストに対応する\n",
    "    # ノードのセクションを解析\n",
    "    node_pattern = re.compile(r\"Node (\\d+): ([^\\n]+)\\nType: ([^\\n]+)(?:\\nQuantity: ([^\\n]+))?\")\n",
    "    nodes = []\n",
    "    for match in node_pattern.finditer(input_string):\n",
    "        groups = match.groups() # example: ('1', '鶏もも肉', 'ingredient', '1枚(200g)')\n",
    "        \n",
    "        # quantityがある場合\n",
    "        if len(groups) == 4:\n",
    "            _, node_id, node_type, quantity = groups\n",
    "            node = {\"id\": node_id, \"type\": node_type, \"quantity\": quantity}\n",
    "        \n",
    "        # quantityがない場合\n",
    "        else:\n",
    "            _, node_id, node_type = groups\n",
    "            node = {\"id\": node_id, \"type\": node_type}\n",
    "        \n",
    "        nodes.append(node)\n",
    "    \n",
    "    # エッジのセクションを解析\n",
    "    edge_pattern = re.compile(r\"Edge \\d+: ([^\\s]+) - ([^\\s]+) ?[\\(（]([^)）]+)[\\)）]\")\n",
    "    edges = []\n",
    "    for match in edge_pattern.finditer(input_string):\n",
    "        source, target, action = match.groups()\n",
    "        edges.append({\"source\": source, \"target\": target, \"action\": action})\n",
    "\n",
    "    # JSONオブジェクトを作成\n",
    "    result = {\n",
    "        \"nodes\": nodes,\n",
    "        \"edges\": edges\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "created_recipe_jsons = []\n",
    "GPT_outputs = os.listdir(\"./data/GPT_messages_outputs\")\n",
    "for _json in GPT_outputs:\n",
    "    with open(f\"./data/GPT_messages_outputs/{_json}\",\"r\",encoding=\"utf-8\") as f:\n",
    "        _json = json.load(f)\n",
    "        _json = parse_to_json(_json[\"output_str\"])\n",
    "        created_recipe_jsons.append(_json)\n",
    "\n",
    "\n",
    "with open(f\"./data/recipe_graph_jsons/GPT4o.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(created_recipe_jsons,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
